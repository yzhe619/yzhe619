{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('operations').getOrCreate()\n",
    "\n",
    "\n",
    "df = spark.read.csv('high_diamond_ranked_10min.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blueWardsPlaced', 'blueWardsDestroyed', 'blueFirstBlood', 'blueKills', 'blueDeaths', 'blueAssists', 'blueEliteMonsters', 'blueDragons', 'blueHeralds', 'blueTowersDestroyed', 'blueTotalGold', 'blueAvgLevel', 'blueTotalExperience', 'blueTotalMinionsKilled', 'blueTotalJungleMinionsKilled', 'blueGoldDiff', 'blueExperienceDiff', 'blueCSPerMin', 'blueGoldPerMin', 'redWardsPlaced', 'redWardsDestroyed', 'redFirstBlood', 'redKills', 'redDeaths', 'redAssists', 'redEliteMonsters', 'redDragons', 'redHeralds', 'redTowersDestroyed', 'redTotalGold', 'redAvgLevel', 'redTotalExperience', 'redTotalMinionsKilled', 'redTotalJungleMinionsKilled', 'redGoldDiff', 'redExperienceDiff', 'redCSPerMin', 'redGoldPerMin', 'blueWins']\n",
      "39 9879\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "print(len(df.columns),df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "v1=df.rdd.map(lambda x:x[3]).collect()\n",
    "plt.hist(v1, color='blue')\n",
    "plt.show()\n",
    "\n",
    "v2=df.rdd.map(lambda x:x[38]).collect()\n",
    "plt.hist(v2, color='blue')\n",
    "plt.show()\n",
    "\n",
    "v3=df.rdd.map(lambda x:x[2]).collect()\n",
    "plt.hist(v3, color='blue')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 9879\n"
     ]
    }
   ],
   "source": [
    "dropatt=[\"redFirstBlood\",\"redGoldDiff\"]\n",
    "df_drop=df.drop(*dropatt)\n",
    "print(len(df_drop.columns),df_drop.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 8392\n"
     ]
    }
   ],
   "source": [
    "df_filter1=df_drop.filter((df_drop.blueKills <= 15)&(df_drop.blueGoldDiff<3500)&(df_drop.blueGoldDiff>-3500))\n",
    "print(len(df_filter1.columns),df_filter1.count())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+------------------------+---------------------+-------------------+------------------+-----------------------+\n",
      "|summary| bluegoldhigh>18000|    bluelevelhigh>7|blueexperiencehigh>18000|bluegolddiffhigh>2000|  redgoldhigh>18000|    redlevelhigh>7|redexperiencehigh>18000|\n",
      "+-------+-------------------+-------------------+------------------------+---------------------+-------------------+------------------+-----------------------+\n",
      "|  count|               8392|               8392|                    8392|                 8392|               8392|              8392|                   8392|\n",
      "|   mean|0.10927073403241182| 0.5425405147759771|      0.4849857006673022|  0.14954718779790277|0.10581506196377502|0.5580314585319351|     0.4915395614871306|\n",
      "| stddev|0.31199717967540275|0.49821670260872386|      0.4998042994779815|   0.3566482629594735| 0.3076191001588246|0.4966505233125514|    0.49995820452957657|\n",
      "|    min|                  0|                  0|                       0|                    0|                  0|                 0|                      0|\n",
      "|    max|                  1|                  1|                       1|                    1|                  1|                 1|                      1|\n",
      "+-------+-------------------+-------------------+------------------------+---------------------+-------------------+------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as pyfunc\n",
    "df_filter2=df_filter1.withColumn(\"bluegoldhigh>18000\",pyfunc.when(df_filter1.blueTotalGold< 18000,0).otherwise(1))\n",
    "df_filter3=df_filter2.withColumn(\"bluelevelhigh>7\",pyfunc.when(df_filter2.blueAvgLevel < 7,0).otherwise(1))\n",
    "df_filter4=df_filter3.withColumn(\"blueexperiencehigh>18000\",pyfunc.when(df_filter3.blueTotalExperience<18000,0).otherwise(1))\n",
    "df_filter5=df_filter4.withColumn(\"bluegolddiffhigh>2000\",pyfunc.when(df_filter4.blueGoldDiff <2000,0).otherwise(1))\n",
    "\n",
    "df_filter6=df_filter5.withColumn(\"redgoldhigh>18000\",pyfunc.when(df_filter5.redTotalGold< 18000,0).otherwise(1))\n",
    "df_filter7=df_filter6.withColumn(\"redlevelhigh>7\",pyfunc.when(df_filter6.redAvgLevel < 7,0).otherwise(1))\n",
    "df_filter8=df_filter7.withColumn(\"redexperiencehigh>18000\",pyfunc.when(df_filter7.redTotalExperience<18000,0).otherwise(1))\n",
    "\n",
    "df_filter8.select(*[\"bluegoldhigh>18000\",\"bluelevelhigh>7\",\"blueexperiencehigh>18000\",\"bluegolddiffhigh>2000\",\"redgoldhigh>18000\",\"redlevelhigh>7\",\"redexperiencehigh>18000\"]).describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.003021990701087143,\n",
       " 0.031661367603869145,\n",
       " 0.13848591013372563,\n",
       " 0.232377735253931,\n",
       " -0.23590672393762963,\n",
       " 0.1832807934930166,\n",
       " 0.17375317597153447,\n",
       " 0.17655949398197546,\n",
       " 0.05871885268587442,\n",
       " 0.04282587180378213,\n",
       " 0.3050986899805216,\n",
       " 0.25372859359725003,\n",
       " 0.2906941350598277,\n",
       " 0.136415036564105,\n",
       " 0.10100642634560494,\n",
       " 0.42926504004593397,\n",
       " 0.3991095209715218,\n",
       " 0.13641503656410484,\n",
       " 0.3050986899805199,\n",
       " -0.01873856807684418,\n",
       " -0.041004850747467486,\n",
       " -0.23590672393762963,\n",
       " 0.232377735253931,\n",
       " -0.18019440728374483,\n",
       " -0.18047575775953742,\n",
       " -0.1757741642635022,\n",
       " -0.06885522778602239,\n",
       " -0.028482035565778687,\n",
       " -0.30016378632731855,\n",
       " -0.2533933521110137,\n",
       " -0.2869026565973431,\n",
       " -0.1227211626526542,\n",
       " -0.08006705715133035,\n",
       " -0.3991095209715218,\n",
       " -0.12272116265265379,\n",
       " -0.30016378632731827,\n",
       " 1.0,\n",
       " 0.16105260073738414,\n",
       " 0.20767759428593607,\n",
       " 0.2376930263327145,\n",
       " 0.2793015291197216,\n",
       " -0.15393484791169448,\n",
       " -0.21490614010690864,\n",
       " -0.23360136926551292]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[df_filter8.corr(\"blueWins\",x) for x in df_filter8.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blueWardsPlaced',\n",
       " 'blueWardsDestroyed',\n",
       " 'blueFirstBlood',\n",
       " 'blueKills',\n",
       " 'blueDeaths',\n",
       " 'blueAssists',\n",
       " 'blueEliteMonsters',\n",
       " 'blueDragons',\n",
       " 'blueHeralds',\n",
       " 'blueTowersDestroyed',\n",
       " 'blueTotalGold',\n",
       " 'blueAvgLevel',\n",
       " 'blueTotalExperience',\n",
       " 'blueTotalMinionsKilled',\n",
       " 'blueTotalJungleMinionsKilled',\n",
       " 'blueGoldDiff',\n",
       " 'blueExperienceDiff',\n",
       " 'blueCSPerMin',\n",
       " 'blueGoldPerMin',\n",
       " 'redWardsPlaced',\n",
       " 'redWardsDestroyed',\n",
       " 'redKills',\n",
       " 'redDeaths',\n",
       " 'redAssists',\n",
       " 'redEliteMonsters',\n",
       " 'redDragons',\n",
       " 'redHeralds',\n",
       " 'redTowersDestroyed',\n",
       " 'redTotalGold',\n",
       " 'redAvgLevel',\n",
       " 'redTotalExperience',\n",
       " 'redTotalMinionsKilled',\n",
       " 'redTotalJungleMinionsKilled',\n",
       " 'redExperienceDiff',\n",
       " 'redCSPerMin',\n",
       " 'redGoldPerMin',\n",
       " 'blueWins',\n",
       " 'bluegoldhigh>18000',\n",
       " 'bluelevelhigh>7',\n",
       " 'blueexperiencehigh>18000',\n",
       " 'bluegolddiffhigh>2000',\n",
       " 'redgoldhigh>18000',\n",
       " 'redlevelhigh>7',\n",
       " 'redexperiencehigh>18000']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filter8.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 8392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.031661367603869145,\n",
       " 0.13848591013372563,\n",
       " 0.232377735253931,\n",
       " 0.1832807934930166,\n",
       " 0.17375317597153447,\n",
       " 0.17655949398197546,\n",
       " 0.05871885268587442,\n",
       " 0.04282587180378213,\n",
       " 0.3050986899805216,\n",
       " 0.25372859359725003,\n",
       " 0.2906941350598277,\n",
       " 0.136415036564105,\n",
       " 0.10100642634560494,\n",
       " 0.42926504004593397,\n",
       " 0.3991095209715218,\n",
       " 0.13641503656410484,\n",
       " 0.3050986899805199,\n",
       " 0.232377735253931,\n",
       " 1.0,\n",
       " 0.16105260073738414,\n",
       " 0.20767759428593607,\n",
       " 0.2376930263327145,\n",
       " 0.2793015291197216]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropatt2=[\"redexperiencehigh>18000\",\"redlevelhigh>7\",\"redgoldhigh>18000\",\"redGoldPerMin\",\"redCSPerMin\",\"redExperienceDiff\",\"redTotalJungleMinionsKilled\",\"redTotalMinionsKilled\",\"redTotalExperience\",\"redAvgLevel\",\"redTotalGold\",\"redTowersDestroyed\",\"redHeralds\",\"redDragons\",\"redEliteMonsters\",\"redAssists\",\"redKills\",\"redWardsDestroyed\",\"redWardsPlaced\",\"blueWardsPlaced\",\"blueDeaths\"]\n",
    "df_drop2=df_filter8.drop(*dropatt2)\n",
    "print(len(df_drop2.columns),df_drop2.count())\n",
    "[df_drop2.corr(\"blueWins\",x) for x in df_drop2.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blueWardsDestroyed',\n",
       " 'blueFirstBlood',\n",
       " 'blueKills',\n",
       " 'blueAssists',\n",
       " 'blueEliteMonsters',\n",
       " 'blueDragons',\n",
       " 'blueHeralds',\n",
       " 'blueTowersDestroyed',\n",
       " 'blueTotalGold',\n",
       " 'blueAvgLevel',\n",
       " 'blueTotalExperience',\n",
       " 'blueTotalMinionsKilled',\n",
       " 'blueTotalJungleMinionsKilled',\n",
       " 'blueGoldDiff',\n",
       " 'blueExperienceDiff',\n",
       " 'blueCSPerMin',\n",
       " 'blueGoldPerMin',\n",
       " 'redDeaths',\n",
       " 'blueWins',\n",
       " 'bluegoldhigh>18000',\n",
       " 'bluelevelhigh>7',\n",
       " 'blueexperiencehigh>18000',\n",
       " 'bluegolddiffhigh>2000']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_drop2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4193"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_drop2.filter((df_drop2.blueWins == 1)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4199"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_drop2.filter((df_drop2.blueWins == 0)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blueWardsDestroyed',\n",
       " 'blueFirstBlood',\n",
       " 'blueKills',\n",
       " 'blueAssists',\n",
       " 'blueEliteMonsters',\n",
       " 'blueDragons',\n",
       " 'blueHeralds',\n",
       " 'blueTowersDestroyed',\n",
       " 'blueTotalGold',\n",
       " 'blueAvgLevel',\n",
       " 'blueTotalExperience',\n",
       " 'blueTotalMinionsKilled',\n",
       " 'blueTotalJungleMinionsKilled',\n",
       " 'blueGoldDiff',\n",
       " 'blueExperienceDiff',\n",
       " 'blueCSPerMin',\n",
       " 'blueGoldPerMin',\n",
       " 'redDeaths',\n",
       " 'blueWins',\n",
       " 'bluegoldhigh>18000',\n",
       " 'bluelevelhigh>7',\n",
       " 'blueexperiencehigh>18000',\n",
       " 'bluegolddiffhigh>2000']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_drop2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "  inputCols=['blueWardsDestroyed',\n",
    "             'blueFirstBlood',\n",
    "             'blueKills',\n",
    "             'blueAssists',\n",
    "             'blueEliteMonsters',\n",
    "             'blueDragons',\n",
    "             'blueHeralds',\n",
    "             'blueTowersDestroyed',\n",
    "             'blueTotalGold',\n",
    "             'blueAvgLevel',\n",
    "             'blueTotalExperience',\n",
    "             'blueTotalMinionsKilled',\n",
    "             'blueTotalJungleMinionsKilled',\n",
    "             'blueGoldDiff',\n",
    "             'blueExperienceDiff',\n",
    "             'blueCSPerMin',\n",
    "             'blueGoldPerMin',\n",
    "             'redDeaths',\n",
    "             'bluegoldhigh>18000',\n",
    "             'bluelevelhigh>7',\n",
    "             'blueexperiencehigh>18000',\n",
    "             'bluegolddiffhigh>2000'],\n",
    "              outputCol=\"features\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(df_drop2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitmodel=output.select(\"features\",\"blueWins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|            features|blueWins|\n",
      "+--------------------+--------+\n",
      "|[2.0,1.0,9.0,11.0...|       0|\n",
      "|(22,[0,2,3,8,9,10...|       0|\n",
      "|[0.0,0.0,7.0,4.0,...|       0|\n",
      "|[1.0,0.0,4.0,5.0,...|       0|\n",
      "|[4.0,0.0,6.0,6.0,...|       0|\n",
      "|[0.0,0.0,5.0,6.0,...|       1|\n",
      "|[3.0,1.0,7.0,7.0,...|       1|\n",
      "|(22,[0,2,3,8,9,10...|       0|\n",
      "|[3.0,0.0,7.0,8.0,...|       0|\n",
      "|[1.0,1.0,4.0,5.0,...|       1|\n",
      "|[3.0,1.0,4.0,6.0,...|       0|\n",
      "|[2.0,1.0,11.0,7.0...|       0|\n",
      "|[1.0,1.0,7.0,11.0...|       1|\n",
      "|[3.0,0.0,4.0,1.0,...|       0|\n",
      "|[3.0,1.0,4.0,4.0,...|       1|\n",
      "|(22,[0,2,3,8,9,10...|       0|\n",
      "|[3.0,0.0,3.0,3.0,...|       0|\n",
      "|[4.0,1.0,5.0,11.0...|       1|\n",
      "|[3.0,0.0,5.0,5.0,...|       0|\n",
      "|[3.0,1.0,11.0,15....|       1|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fitmodel.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data = fitmodel.randomSplit([0.9,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier,GBTClassifier,RandomForestClassifier\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(labelCol='blueWins',featuresCol='features')\n",
    "rfc = RandomForestClassifier(labelCol='blueWins',featuresCol='features')\n",
    "gbt = GBTClassifier(labelCol='blueWins',featuresCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_model = dtc.fit(train_data)\n",
    "rfc_model = rfc.fit(train_data)\n",
    "gbt_model = gbt.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_predictions = dtc_model.transform(test_data)\n",
    "rfc_predictions = rfc_model.transform(test_data)\n",
    "gbt_predictions = gbt_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTC\n",
      "0.6626321384279218\n",
      "RFC\n",
      "0.7606980452371095\n",
      "GBT\n",
      "0.6779542904387882\n"
     ]
    }
   ],
   "source": [
    "# Let's start off with binary classification.\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Note that the label column isn't named label, it's named PrivateIndex in this case.\n",
    "my_binary_eval = BinaryClassificationEvaluator(labelCol = 'blueWins')\n",
    "\n",
    "# This is the area under the curve. This indicates that the data is highly seperable.\n",
    "print(\"DTC\")\n",
    "print(my_binary_eval.evaluate(dtc_predictions))\n",
    "\n",
    "# RFC improves accuracy but also model complexity. RFC outperforms DTC in nearly every situation.\n",
    "print(\"RFC\")\n",
    "print(my_binary_eval.evaluate(rfc_predictions))\n",
    "\n",
    "# We can't repeat these exact steps for GBT. If you print the schema of all three, you may be able to notice why.\n",
    "# Instead, let's redefine the object:\n",
    "my_binary_gbt_eval = BinaryClassificationEvaluator(labelCol='blueWins', rawPredictionCol='prediction')\n",
    "print(\"GBT\")\n",
    "print(my_binary_gbt_eval.evaluate(gbt_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"blueWins\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the results!\n",
      "----------------------------------------\n",
      "A single decision tree has an accuracy of: 67.56%\n",
      "----------------------------------------\n",
      "A random forest ensemble has an accuracy of: 68.29%\n",
      "----------------------------------------\n",
      "An ensemble using GBT has an accuracy of: 67.80%\n"
     ]
    }
   ],
   "source": [
    "dtc_acc = acc_evaluator.evaluate(dtc_predictions)\n",
    "rfc_acc = acc_evaluator.evaluate(rfc_predictions)\n",
    "gbt_acc = acc_evaluator.evaluate(gbt_predictions)\n",
    "\n",
    "print(\"Here are the results!\")\n",
    "print('-'*40)\n",
    "print('A single decision tree has an accuracy of: {0:2.2f}%'.format(dtc_acc*100))\n",
    "print('-'*40)\n",
    "print('A random forest ensemble has an accuracy of: {0:2.2f}%'.format(rfc_acc*100))\n",
    "print('-'*40)\n",
    "print('An ensemble using GBT has an accuracy of: {0:2.2f}%'.format(gbt_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = dtc_predictions.filter(dtc_predictions['prediction'] == 1).filter(dtc_predictions['blueWins'] == 1).count()\n",
    "FN = dtc_predictions.filter(dtc_predictions['prediction'] == 0).filter(dtc_predictions['blueWins'] == 1).count()\n",
    "TN = dtc_predictions.filter(dtc_predictions['prediction'] == 0).filter(dtc_predictions['blueWins'] == 0).count()\n",
    "FP = dtc_predictions.filter(dtc_predictions['prediction'] == 1).filter(dtc_predictions['blueWins'] == 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/session.py:340: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "conf_mat = sc.parallelize([\n",
    "    {\"Positive\": TP, \"Negative\": FP,\"Matrix\":\"Positive\"},\n",
    "    {\"Positive\": FN, \"Negative\": TN,\"Matrix\":\"Negative\"}\n",
    "]).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+\n",
      "|  Matrix|Negative|Positive|\n",
      "+--------+--------+--------+\n",
      "|Positive|     145|     291|\n",
      "|Negative|     265|     122|\n",
      "+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conf_mat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
